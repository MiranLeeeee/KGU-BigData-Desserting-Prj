# -*- coding: utf-8 -*-
"""Khaiii_사전_추가.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rGBnKDtvmcorrElEUl8uHZZnXwhSmahN
"""

!pip install konlpy

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!pip3 install JPype1-py3

import os
os.chdir('/tmp/')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.1.tar.gz
!tar zxfv mecab-0.996-ko-0.9.1.tar.gz
os.chdir('/tmp/mecab-0.996-ko-0.9.1')
!./configure
!make
!make check
!make install

os.chdir('/tmp')
!curl -LO http://ftpmirror.gnu.org/automake/automake-1.11.tar.gz
!tar -zxvf automake-1.11.tar.gz
os.chdir('/tmp/automake-1.11')
!./configure
!make
!make install

import os
os.chdir('/tmp/')

 
!wget -O m4-1.4.9.tar.gz http://ftp.gnu.org/gnu/m4/m4-1.4.9.tar.gz
!tar -zvxf m4-1.4.9.tar.gz
os.chdir('/tmp/m4-1.4.9')
!./configure
!make
!make install

os.chdir('/tmp')
!curl -OL http://ftpmirror.gnu.org/autoconf/autoconf-2.69.tar.gz
!tar xzf autoconf-2.69.tar.gz
os.chdir('/tmp/autoconf-2.69')
!./configure --prefix=/usr/local
!make
!make install
!export PATH=/usr/local/bin

import os
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz
!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!./autogen.sh
!./configure
!make
# !sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc'
!make install

os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!ldconfig
!ldconfig -p | grep /usr/local/lib

import os
os.chdir('/tmp')
!curl -LO https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.0.1-20150920.tar.gz
!tar -zxvf mecab-ko-dic-2.0.1-20150920.tar.gz
os.chdir('/tmp/mecab-ko-dic-2.0.1-20150920')
!./autogen.sh
!./configure
!make
# !sh -c 'echo "dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic" > /usr/local/etc/mecabrc'
!make install

# install mecab-python
import os
os.chdir('/content')

!git clone https://bitbucket.org/eunjeon/mecab-python-0.996.git
os.chdir('/content/mecab-python-0.996')

!python3 setup.py build
!python3 setup.py install

# colab에 KoNLPy 설치

!apt-get update
!apt-get install g++ openjdk-8-jdk python-dev python3-dev 
!pip3 install JPype1-py3 
!pip3 install konlpy 
!JAVA_HOME="C:\Program Files\Java\jdk-14.0.2"

! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

cd Mecab-ko-for-Google-Colab

! bash install_mecab-ko_on_colab190912.sh

# Mecab user dictionary 반영
#! /content/mecab-ko-dic-2.1.1-20180720/tools/add-userdic.sh

'''from konlpy.tag import Mecab
mecab = Mecab() # Tagger
n_tags = ['NNG', 'NNP', 'NNB', 'VV', "VA" ] # 일반 명사, 고유 명사, 의존 명사, 동사, 형용사

def extract_corpus_mecab(texts):

  extract_corpus = []
  for line in texts:
    if str(line) != 'nan':
      temp = mecab.pos(str(line))
      nouns = []
      for i in range(len(temp)):
        if temp[i][1] in n_tags:
          nouns.append(temp[i][0])
      extract_corpus.append(nouns)

  return extract_corpus

      '''

#extract_corpus_mecab = extract_corpus_mecab(data['Review'])

#print(len(data['Review']), len(extract_corpus_mecab))

# Colab에 Khaiii 설치
os.chdir('/content')
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

import json # import json module

# with statement
with open('desserting_appdata.json', encoding='utf-8') as json_file:
    json_data = json.load(json_file)

# suple 데이터에서 menu를 뽑아오는 Code
from pprint import pprint

menu = []
error_index = []

affiliates = list(json_data['affiliates'].keys())

for i in range(len(affiliates)):
    
    try :
        temp = affiliates[i]
        menus = list(json_data['affiliates'][temp]['menus'].keys())
        for j in range(len(menus)):
            tem = menus[j]
            menu.append(json_data['affiliates'][temp]['menus'][tem]['title'])
    except:
        error_index.append(i)

menu[:10]

import re 

final_data = []

for line in menu:
  for word in line.split():
    word = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"\^]', '',word)
    word = re.sub('[0-9]','', word)
    word = re.sub('[-a-zA-Z]','', word)
    word = re.sub(r'\s+', ' ', word)
    if len(word) > 1 and word not in final_data:
      final_data.append(word)
    else:
      continue

final = []
for word in final_data:
  temp = str(word)+"*\t"+str(word)+'/NNP'
  final.append(temp)

import pandas as pd

final = pd.DataFrame(final)
final.columns = ['#']

final.to_csv('/content/khaiii/rsc/src/preanal.manual', encoding='utf-8', index=False, header=False)

# Khaiii 사용자 사전 추가
!cd /content/khaiii/rsc
!mkdir -p /content/build/share/khaiii
!PYTHONPATH=/content/khaiii/src/main/python /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

for word in api.analyze('얼그레이가 맛있습니다.'):
  for morphs in word.morphs:
    print(morphs)

data = pd.read_csv('/content/preprocessed_data.csv', encoding='utf-8').drop(['Unnamed: 0'], axis=1)

data.head()

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

n_tags = ['NNG', 'NNP', 'NNB']#, 'VV', "VA" ]
#ex = data['Review'][2]
#bad_ex = data['Review'][4]

def extract_corpus_khaiii(texts):
    extract_corpus = []
    for line in texts:
      if str(line) != 'nan':
        nouns = []

        for word in api.analyze(str(line)):
          for morphs in word.morphs:
            if morphs.tag in n_tags:
              if len(morphs.lex) > 1:
                nouns.append(morphs.lex)
              else:
                continue

        extract_corpus.append(nouns)

    return extract_corpus

extract_corpus_khaiii = extract_corpus_khaiii(data['Review'])

extract_corpus_khaiii

# countvectorize를 위한 역토큰화 진행

detokenized_doc = []

for i in range(len(extract_corpus_khaiii)):
  if extract_corpus_khaiii[i] != []:
    t = ' '.join(extract_corpus_khaiii[i])
    detokenized_doc.append(t)

detokenized_doc[:10]

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=2)

vect = vectorizer.fit_transform(detokenized_doc)

np.sum(vect.toarray()[0])

from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=6, random_state=0)
lda.fit_transform(vect)

terms = vectorizer.get_feature_names()

def get_topics(components, feature_names, n=10):
  temp = []
  topic_result = {}
  for idx, topic in enumerate(components):
      topic_result["Topic "+str(idx+1)] =  [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]]

  return topic_result

def print_topics(components, feature_names, n=10):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], np.log(topic[i]).round(2)) for i in topic.argsort()[:-n - 1:-1]])

print_topics(lda.components_, terms)

np.sum(lda.components_[0])

n_min_df = [0.0001, 0.001, 0.01, 0.1, 1, 2, 3]
n_topic = [6, 8, 10, 12]

result = pd.DataFrame()

for min_df in n_min_df:
  temp_topic = {}
  vectorizer = CountVectorizer(min_df=min_df)
  vect = vectorizer.fit_transform(detokenized_doc)
  terms = vectorizer.get_feature_names()

  for topics in n_topic:
    lda = LatentDirichletAllocation(n_components=topics)
    lda.fit_transform(vect.toarray())
    temp_topic = get_topics(lda.components_, terms)
    temp_topic['n_df'] = min_df
    temp_topic['n_topic'] = topics


    temp = pd.DataFrame.from_dict(temp_topic)
    result = pd.concat([result, temp], axis=0)

result.keys()

result = result[['n_df', 'n_topic', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6','Topic 7', 'Topic 8', 'Topic 9', 'Topic 10','Topic 11', 'Topic 12']]

result

result.to_csv('Topic 결과.csv', encoding='utf-8')

