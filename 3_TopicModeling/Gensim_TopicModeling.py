# -*- coding: utf-8 -*-
"""Gensim_Topic_Modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ND_KD08fVP5vv2V5FURf7AD8XNlfkmRg
"""

# Colab에 Khaiii 설치
import os
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

import json # import json module
import os
os.chdir('/content')

# with statement
with open('/content/desserting_appdata.json', encoding='utf-8') as json_file:
    json_data = json.load(json_file)

# suple 데이터에서 menu를 뽑아오는 Code

menu = []
error_index = []

affiliates = list(json_data['affiliates'].keys())

for i in range(len(affiliates)):
    
    try :
        temp = affiliates[i]
        menus = list(json_data['affiliates'][temp]['menus'].keys())
        for j in range(len(menus)):
            tem = menus[j]
            menu.append(json_data['affiliates'][temp]['menus'][tem]['title'])
    except:
        error_index.append(i)

menu[:10]

import re 

final_data = []

for line in menu:
  for word in line.split():
    word = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"\^]', '',word)
    word = re.sub('[0-9]','', word)
    word = re.sub('[-a-zA-Z]','', word)
    word = re.sub(r'\s+', ' ', word)
    if len(word) <10 and len(word) > 1 and word not in final_data:
      final_data.append(word)
    else:
      continue

final = []
for word in final_data:
  temp = str(word)+"*\t"+str(word)+'/NNP'
  final.append(temp)

import pandas as pd

final = pd.DataFrame(final)
final.columns = ['#']

final.to_csv('/content/khaiii/rsc/src/preanal.manual', encoding='utf-8', index=False, header=False)

# Khaiii 사용자 사전 추가
!cd /content/khaiii/rsc
!mkdir -p /content/build/share/khaiii
!PYTHONPATH=/content/khaiii/src/main/python /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

# Testing

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

for word in api.analyze('얼그레이가 맛있습니다.'):
  for morphs in word.morphs:
    print(morphs)

"""## Gensim Topic Modeling"""

data = pd.read_csv('/content/groupby_data.csv', encoding='utf-8')

data.info()

"""# Gensim LDA를 위한 데이터 전처리
## Experiment 1) Khaiii에서 명사, 어근만 추출해 Tokenizing
"""

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

n_tags = ['NNG', 'NNP', 'NNB', 'XR']#, 'VV', "VA", "XR"]

def extract_corpus_khaiii(texts):
    extract_corpus = []
    for line in texts:
      if str(line) != 'nan':
        nouns = []

        for word in api.analyze(str(line)):
          for morphs in word.morphs:
            if morphs.tag in n_tags:
              if len(morphs.lex) > 1:
                nouns.append(morphs.lex)
              else:
                continue

        extract_corpus.append(nouns)

    return extract_corpus

khaiii_xr = extract_corpus_khaiii(data['Review'])

print(len(data), len(khaiii_xr))

# countvectorize를 위한 역토큰화 진행

def detokenize(token_list):
  detokenized_doc = []
  for i in range(len(token_list)):
    if token_list[i] != []:
      t = ' '.join(token_list[i])

      detokenized_doc.append(t)
  return detokenized_doc
    #detokenized_doc.append([data['Nickname'][i], t]) -> 옆에 닉네임 붙여서 내보낼거면 활성화

detoken_xr = detokenize(khaiii_xr)

# 모든 토큰을 한 줄로 만듦
import numpy as np

#detoken_data = [" ".join(detoken_xr)]

#detoken_data = np.array(detoken_data)

# 닉네임 붙여서 내보낼거면 활성화
#df = pd.DataFrame(detokenized_doc, columns=['Nickname', 'detoken Review'])
#df.to_csv('Final TDM.csv', encoding='utf-8')

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()

cv = vectorizer.fit_transform(detoken_xr)

cv.shape

"""## Using Gensim"""

# install gensim

!pip install gensim

# 단어문서행렬을 gensim 형태로 변환

from gensim.matutils import Sparse2Corpus

corpus = Sparse2Corpus(cv.T)

corpus[0]

# 단어 번호와 단어를 사전으로 정리

id2word = dict(enumerate(vectorizer.get_feature_names()))

from gensim.models.ldamodel import LdaModel
from gensim.models.coherencemodel import CoherenceModel

"""* **num_topics**: 최종 분석의 주제 수
* **passes**: 총 훈련 과정의 수. 신경망 학습에서 에포크와 같다
* **iteration** : 각 문서에 대해 업데이트를 반복하는 횟수
* **random_state**: 재현 가능한 결과를 위해 임의의 숫자를 설정한다.
"""

model = LdaModel(corpus=corpus,
                 id2word=id2word,
                 num_topics=100, 
                 iterations=500,
                 passes=10)

# Coherence 계산을 위한 dictionary 만들기

from gensim.corpora.dictionary import Dictionary
dic = Dictionary()
dic.token2id = {t: i for i, t in enumerate(vectorizer.get_feature_names())}

coherence_model_lda = CoherenceModel(model=model, texts=khaiii_xr, dictionary=dic, coherence='c_v')

coherence = coherence_model_lda.get_coherence()

coherence.round(2)

"""### 최적의 number of words 찾기
* 지표 : Coherence
"""

# vector 생성할 때마다 dictionary 만드는 방법

coherence_value = {}

#dic = Dictionary()
#dic.token2id = {t: i for i, t in enumerate(vectorizer.get_feature_names())}

for features in range(1000, 33000, 1000):
  print("{}'s of words training".format(features))

  vect = CountVectorizer(max_features=features)
  count_vect = vect.fit_transform(detoken_xr)

  corpus = Sparse2Corpus(count_vect.T)

  naive_model = LdaModel(corpus=corpus, id2word=id2word)

  dic = Dictionary()
  dic.token2id = {t: i for i, t in enumerate(vect.get_feature_names())}

  naive_cv_lda = CoherenceModel(model=naive_model, texts=khaiii_xr, dictionary=dic, coherence='c_v')
  c_v = naive_cv_lda.get_coherence()
  coherence_value[features] = c_v

new_coherence_value = {}

dic = Dictionary()
dic.token2id = {t: i for i, t in enumerate(vectorizer.get_feature_names())}

for features in range(1000, 33000, 1000):
  print("{}'s of words training".format(features))

  vect = CountVectorizer(max_features=features)
  count_vect = vect.fit_transform(detoken_xr)

  corpus = Sparse2Corpus(count_vect.T)

  naive_model = LdaModel(corpus=corpus, id2word=id2word)

  naive_cv_lda = CoherenceModel(model=naive_model, texts=khaiii_xr, dictionary=dic, coherence='c_v')
  new_coherence_value[features] = naive_cv_lda.get_coherence()

"""#### Number Of Words 값의 변화에 따른 Coherence 값 시각화"""

import matplotlib.pyplot as plt

# 글씨 크기 조정
plt.rc('font', size=14)

num_words = range(1000, 33000, 1000)
c_v = list(coherence_value.values())

plt.figure(figsize=(15, 10))

plt.xlabel('The Number of words')
plt.ylabel('Coherence Value')
plt.xticks(np.arange(1000, 33000, 1000), rotation=45)
plt.ylim(0.31, 0.34)
plt.title('Cohernce Value for chaing # of word \n: changing every time Dictionary', fontsize=20)
#plt.yticks(np.arange(0.30, 0.35, 0.01))
plt.scatter(num_words, c_v)
plt.plot(num_words, c_v)
plt.grid()
plt.show()

new_num = range(1000, 33000, 1000)
new_c_v = list(new_coherence_value.values())

plt.figure(figsize=(15, 10))
plt.title('Cohernce Value for chaing # of word \n: fixing Dictionary', fontsize=20)
plt.xlabel('The Number of words')
plt.ylabel('Coherence Value')
#plt.ylim(0.31, 0.34)
plt.xticks(np.arange(1000, 33000, 1000), rotation=45)
#plt.yticks(np.arange(0.30, 0.35, 0.01))
plt.scatter(new_num, new_c_v, c='r')
plt.plot(new_num, new_c_v, 'r')
plt.grid()
plt.show()

"""### 문서의 주제 확인"""

n_min_df = [0.0001, 0.001, 0.01, 0.1, 1, 2, 3]


result = pd.DataFrame()

for min_df in n_min_df:
  temp_topic = {}
  vectorizer = CountVectorizer(min_df=min_df)
  vect = vectorizer.fit_transform(detokenized_doc)
  terms = vectorizer.get_feature_names()

  for topics in n_topic:
    lda = LatentDirichletAllocation(n_components=topics)
    lda.fit_transform(vect.toarray())
    temp_topic = get_topics(lda.components_, terms)
    temp_topic['n_df'] = min_df
    temp_topic['n_topic'] = topics


    temp = pd.DataFrame.from_dict(temp_topic)
    result = pd.concat([result, temp], axis=0)

result = result[['n_df', 'n_topic', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6','Topic 7', 'Topic 8', 'Topic 9', 'Topic 10','Topic 11', 'Topic 12']]

result.to_csv('Topic 결과_final.csv', encoding='utf-8')