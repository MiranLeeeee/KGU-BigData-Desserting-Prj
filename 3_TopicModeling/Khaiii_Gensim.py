# -*- coding: utf-8 -*-
"""Khaiii_사용자_사전_추가_및_Topic_Modeling v0.2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a2T6n7XtX8z8ifAVRuw5X__Gk28X4q5A
"""

# Colab에 Khaiii 설치
import os
os.chdir('/content')
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

import json # import json module
import os
os.chdir('/content')

# with statement
with open('/content/desserting_appdata.json', encoding='utf-8') as json_file:
    json_data = json.load(json_file)

# suple 데이터에서 menu를 뽑아오는 Code

menu = []
error_index = []

affiliates = list(json_data['affiliates'].keys())

for i in range(len(affiliates)):
    
    try :
        temp = affiliates[i]
        menus = list(json_data['affiliates'][temp]['menus'].keys())
        for j in range(len(menus)):
            tem = menus[j]
            menu.append(json_data['affiliates'][temp]['menus'][tem]['title'])
    except:
        error_index.append(i)

menu[:10]

import re 

final_data = []

for line in menu:
  for word in line.split():
    word = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"\^]', '',word)
    word = re.sub('[0-9]','', word)
    word = re.sub('[-a-zA-Z]','', word)
    word = re.sub(r'\s+', ' ', word)
    if len(word) <10 and len(word) > 1 and word not in final_data:
      final_data.append(word)
    else:
      continue

final = []
for word in final_data:
  temp = str(word)+"*\t"+str(word)+'/NNP'
  final.append(temp)

import pandas as pd

final = pd.DataFrame(final)
final.columns = ['#']

final.to_csv('/content/khaiii/rsc/src/preanal.manual', encoding='utf-8', index=False, header=False)

# Khaiii 사용자 사전 추가
!cd /content/khaiii/rsc
!mkdir -p /content/build/share/khaiii
!PYTHONPATH=/content/khaiii/src/main/python /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

# Testing

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

for word in api.analyze('얼그레이가 맛있습니다.'):
  for morphs in word.morphs:
    print(morphs)

"""## Gensim Topic Modeling"""

data = pd.read_csv('/content/groupby_data.csv', encoding='utf-8')

data.info()

"""# Gensim LDA를 위한 데이터 전처리
## Experiment 1) Khaiii에서 명사, 어근만 추출해 Tokenizing
"""

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

n_tags = ['NNG', 'NNP', 'NNB', 'XR']#, 'VV', "VA", "XR"]

def extract_corpus_khaiii(texts):
    extract_corpus = []
    for line in texts:
      if str(line) != 'nan':
        nouns = []

        for word in api.analyze(str(line)):
          for morphs in word.morphs:
            if morphs.tag in n_tags:
              if len(morphs.lex) > 1:
                nouns.append(morphs.lex)
              else:
                continue

        extract_corpus.append(nouns)

    return extract_corpus

khaiii_xr = extract_corpus_khaiii(data['Review'])

print(len(data), len(khaiii_xr))

khaiii_xr

# countvectorize를 위한 역토큰화 진행

def detokenize(token_list):
  detokenized_doc = []
  for i in range(len(token_list)):
    if token_list[i] != []:
      t = ' '.join(token_list[i])

      detokenized_doc.append(t)
  return detokenized_doc
    #detokenized_doc.append([data['Nickname'][i], t]) -> 옆에 닉네임 붙여서 내보낼거면 활성화

detoken_xr = detokenize(khaiii_xr)

detoken_xr

# 모든 토큰을 한 줄로 만듦
import numpy as np

detoken_data = [" ".join(detoken_xr)]

detoken_data = np.array(detoken_data)

detoken_data

# 닉네임 붙여서 내보낼거면 활성화
#df = pd.DataFrame(detokenized_doc, columns=['Nickname', 'detoken Review'])
#df.to_csv('Final TDM.csv', encoding='utf-8')

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_features=10000)

cv = vectorizer.fit_transform(detoken_xr)

"""## Using Gensim"""

# install gensim

!pip install gensim

cv.shape

# 단어문서행렬을 gensim 형태로 변환

from gensim.matutils import Sparse2Corpus

corpus = Sparse2Corpus(cv.T)

corpus[0]

# 단어 번호와 단어를 사전으로 정리

id2word = dict(enumerate(vectorizer.get_feature_names()))

id2word

from gensim.models.ldamodel import LdaModel

"""* **num_topics**: 최종 분석의 주제 수
* **passes**: 총 훈련 과정의 수. 신경망 학습에서 에포크와 같다
* **iteration** : 각 문서에 대해 업데이트를 반복하는 횟수
* **random_state**: 재현 가능한 결과를 위해 임의의 숫자를 설정한다.
"""

def compute_coherence_values(corpus, dictionary, k, a, b):
    
    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=10, 
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=a,
                                           eta=b,
                                           per_word_topics=True)
    
    coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=id2word, coherence='c_v')
    
    return coherence_model_lda.get_coherence()

model = LdaModel(corpus=corpus,
                 id2word=id2word,
                 iterations=500,
                 passes=10,
                 num_topics=10)

model.show_topics()

"""### 문서의 주제 확인"""

# 테스트 데이터를 gensim 형식으로 변환

test_corpus = Sparse2Corpus(x_test.T)

# 첫번째 문서를 확인
doc = test_corpus[0]
doc

model.get_document_topics(doc)

"""### 혼란도(Perplexity)

혼란도는 낮을 수록, ```log_perplexity```는 높을 수록 좋음
"""

# log_perplexity 메소드로 - H에 해당하는 값을 구할 수 있음

log_pp = model.log_perplexity(test_corpus)

# 혼란도로 바꾸려면 아래와 같이 계산

2 ** -log_pp

# 각각 print

print("log_perplexity : {:.3f} \n혼란도 : {:.3f}".format(log_pp, 2*-log_pp))

plots = df.loc[index_test, 'Review']

import re

token_re = re.compile(cv.token_pattern)

words = set(cv.get_feature_names())
texts = []

for plot in plots:
  text = []
  for word in token_re.findall(plot):
    if word in word:
      text.append(word)
  texts.append(text)

from gensim.corpora.dictionary import Dictionary

n_min_df = [0.0001, 0.001, 0.01, 0.1, 1, 2, 3]


result = pd.DataFrame()

for min_df in n_min_df:
  temp_topic = {}
  vectorizer = CountVectorizer(min_df=min_df)
  vect = vectorizer.fit_transform(detokenized_doc)
  terms = vectorizer.get_feature_names()

  for topics in n_topic:
    lda = LatentDirichletAllocation(n_components=topics)
    lda.fit_transform(vect.toarray())
    temp_topic = get_topics(lda.components_, terms)
    temp_topic['n_df'] = min_df
    temp_topic['n_topic'] = topics


    temp = pd.DataFrame.from_dict(temp_topic)
    result = pd.concat([result, temp], axis=0)

result.keys()

result = result[['n_df', 'n_topic', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6','Topic 7', 'Topic 8', 'Topic 9', 'Topic 10','Topic 11', 'Topic 12']]

result

result.to_csv('Topic 결과_final.csv', encoding='utf-8')