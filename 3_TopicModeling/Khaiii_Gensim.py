# -*- coding: utf-8 -*-
"""Khaiii_사용자_사전_추가_및_Topic_Modeling v0.4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13jiKvVsVW-48lufOAXWsyxoZ_CZzwvBr
"""

# Colab에 Khaiii 설치
import os
os.chdir('/content')
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

import json # import json module

# with statement
with open('desserting_appdata.json', encoding='utf-8') as json_file:
    json_data = json.load(json_file)

# suple 데이터에서 menu를 뽑아오는 Code
from pprint import pprint

menu = []
error_index = []

affiliates = list(json_data['affiliates'].keys())

for i in range(len(affiliates)):
    
    try :
        temp = affiliates[i]
        menus = list(json_data['affiliates'][temp]['menus'].keys())
        for j in range(len(menus)):
            tem = menus[j]
            menu.append(json_data['affiliates'][temp]['menus'][tem]['title'])
    except:
        error_index.append(i)

menu[:10]

import re 

final_data = []

for line in menu:
  for word in line.split():
    word = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"\^]', '',word)
    word = re.sub('[0-9]','', word)
    word = re.sub('[-a-zA-Z]','', word)
    word = re.sub(r'\s+', ' ', word)
    if len(word) <10 and len(word) > 1 and word not in final_data:
      final_data.append(word)
    else:
      continue

final = []
for word in final_data:
  temp = str(word)+"*\t"+str(word)+'/NNP'
  final.append(temp)

import pandas as pd

final = pd.DataFrame(final)
final.columns = ['#']

final.to_csv('/content/khaiii/rsc/src/preanal.manual', encoding='utf-8', index=False, header=False)

# Khaiii 사용자 사전 추가
!cd /content/khaiii/rsc
!mkdir -p /content/build/share/khaiii
!PYTHONPATH=/content/khaiii/src/main/python /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

for word in api.analyze('얼그레이가 맛있습니다.'):
  for morphs in word.morphs:
    print(morphs)

data = pd.read_csv('/content/final_preprocessed_data.csv', encoding='utf-8', index_col=0)

data.head()

data.info()

"""## Experiment 1) Khaiii에서 네이버 플레이스 리뷰 명사만 추출해 Topic Modeling"""

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

n_tags = ['NNG', 'NNP', 'NNB']#, 'VV', "VA", "XR"]
#ex = data['Review'][2]
#bad_ex = data['Review'][4]

def extract_corpus_khaiii(texts):
    extract_corpus = []
    for line in texts:
      if str(line) != 'nan':
        nouns = []

        for word in api.analyze(str(line)):
          for morphs in word.morphs:
            if morphs.tag in n_tags:
              if len(morphs.lex) > 1:
                nouns.append(morphs.lex)
              else:
                continue

        extract_corpus.append(nouns)

    return extract_corpus

khaiii_noun = extract_corpus_khaiii(data['Review'])

print(len(data), len(khaiii_noun))

#data['preprocessed'] = extract_corpus_khaiii

# countvectorize를 위한 역토큰화 진행

def detokenize(token_list):
  detokenized_doc = []
  for i in range(len(token_list)):
    if token_list[i] != []:
      t = ' '.join(token_list[i])
      detokenized_doc.append(t)
  return detokenized_doc
    #detokenized_doc.append([data['Nickname'][i], t]) -> 옆에 닉네임 붙여서 내보낼거면 활성화

noun_detoken = detokenize(khaiii_noun)

noun_detoken[:10]

# 닉네임 붙여서 내보낼거면 활성화
#df = pd.DataFrame(detokenized_doc, columns=['Nickname', 'detoken Review'])
#df.to_csv('Final TDM.csv', encoding='utf-8')

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=2)

vect = vectorizer.fit_transform(noun_detoken)

from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

lda = LatentDirichletAllocation(n_components=6, random_state=0, max_iter=100,  learning_method='online')
lda.fit_transform(vect)

terms = vectorizer.get_feature_names()

def get_topics(components, feature_names, n=10):
  temp = []
  topic_result = {}
  for idx, topic in enumerate(components):
      topic_result["Topic "+str(idx+1)] =  [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]]

  return topic_result

def print_topics(components, feature_names, n=10):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])

print_topics(lda.components_, terms)

"""## Experiment 2 ) Khaiii에서 명사, 형용사, 동사, 어근을 추출해 Topic Modeling"""

n_tags = ['NNG', 'NNP', 'NNB', 'VV', "VA", "XR"]

included_khaiii = extract_corpus_khaiii(data['Review'])

included_khaiii[:10]

all_detoken = detokenize(included_khaiii)

all_detoken[:10]

# 명사, 동사, 형용사, 어근을 추출한 list를 역토큰하여 벡터화

all_vect = vectorizer.fit_transform(all_detoken)

lda_exp_2 = LatentDirichletAllocation(n_components=6, random_state=0, max_iter=100,  learning_method='online')
lda_all = lda_exp_2.fit_transform(all_vect)

all_terms = vectorizer.get_feature_names()

print_topics(lda_exp_2.components_, all_terms)

n_tags = ['NNG', 'NNP', 'NNB', "XR"]

only_XR_khaiii = extract_corpus_khaiii(data['Review'])

detoken_xr = detokenize(only_XR_khaiii)

xr_vect = vectorizer.fit_transform(detoken_xr)

lda_exp_3 = LatentDirichletAllocation(n_components=6, random_state=0, max_iter=100,  learning_method='online')
lda_xr = lda_exp_3.fit_transform(xr_vect)

xr_terms = vectorizer.get_feature_names()

lda_exp_3_component = lda_exp_3.components_ / lda_exp_3.components_.sum(axis=1)[:, np.newaxis]

np.sum(lda_exp_3_component[0])

print_topics(lda_exp_3_component, xr_terms)

n_min_df = [0.0001, 0.001, 0.01, 0.1, 1, 2, 3]
n_topic = [6, 8, 10, 12]

result = pd.DataFrame()

for min_df in n_min_df:
  temp_topic = {}
  vectorizer = CountVectorizer(min_df=min_df)
  vect = vectorizer.fit_transform(detokenized_doc)
  terms = vectorizer.get_feature_names()

  for topics in n_topic:
    lda = LatentDirichletAllocation(n_components=topics)
    lda.fit_transform(vect.toarray())
    temp_topic = get_topics(lda.components_, terms)
    temp_topic['n_df'] = min_df
    temp_topic['n_topic'] = topics


    temp = pd.DataFrame.from_dict(temp_topic)
    result = pd.concat([result, temp], axis=0)

result.keys()

result = result[['n_df', 'n_topic', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6','Topic 7', 'Topic 8', 'Topic 9', 'Topic 10','Topic 11', 'Topic 12']]

result

result.to_csv('Topic 결과_final.csv', encoding='utf-8')