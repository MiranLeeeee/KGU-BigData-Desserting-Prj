# -*- coding: utf-8 -*-
"""Khaiii_사전_추가_gensim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PPugPZ1IzvuBS9TrSm_Bt3zpwKiSsJJ9
"""

# Colab에 Khaiii 설치
import os
os.chdir('/content')
!git clone https://github.com/kakao/khaiii.git
!pip install cmake
!mkdir build
!cd build && cmake /content/khaiii
!cd /content/build/ && make all
!cd /content/build/ && make resource
!cd /content/build && make install
!cd /content/build && make package_python
!pip install /content/build/package_python

import json # import json module

# with statement
with open('desserting_appdata.json', encoding='utf-8') as json_file:
    json_data = json.load(json_file)

# suple 데이터에서 menu를 뽑아오는 Code
from pprint import pprint

menu = []
error_index = []

affiliates = list(json_data['affiliates'].keys())

for i in range(len(affiliates)):
    
    try :
        temp = affiliates[i]
        menus = list(json_data['affiliates'][temp]['menus'].keys())
        for j in range(len(menus)):
            tem = menus[j]
            menu.append(json_data['affiliates'][temp]['menus'][tem]['title'])
    except:
        error_index.append(i)

menu[:10]

import re 

final_data = []

for line in menu:
  for word in line.split():
    word = re.sub(r'[@%\\*=()/~#&\+á?\xc3\xa1\-\|\.\:\;\!\-\,\_\~\$\'\"\^]', '',word)
    word = re.sub('[0-9]','', word)
    word = re.sub('[-a-zA-Z]','', word)
    word = re.sub(r'\s+', ' ', word)
    if len(word)<10 and len(word) > 1 and word not in final_data:
      final_data.append(word)
    else:
      continue

from google.colab import drive
drive.mount('/content/drive')

final = []
for word in final_data:
  temp = str(word)+"*\t"+str(word)+'/NNP'
  final.append(temp)

import pandas as pd

final = pd.DataFrame(final)
final.columns = ['#']

final.to_csv('/content/khaiii/rsc/src/preanal.manual', encoding='utf-8', index=False, header=False)

# Khaiii 사용자 사전 추가
!cd /content/khaiii/rsc
!mkdir -p /content/build/share/khaiii
!PYTHONPATH=/content/khaiii/src/main/python /content/khaiii/rsc/bin/compile_preanal.py --rsc-src=/content/khaiii/rsc/src --rsc-dir=/content/build/share/khaiii

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

for word in api.analyze('얼그레이가 맛있습니다.'):
  for morphs in word.morphs:
    print(morphs)

data = pd.read_csv('/content/final_preprocessed_data.csv', encoding='utf-8').drop(['Unnamed: 0'], axis=1)

data.head()

from khaiii import KhaiiiApi
api = KhaiiiApi(rsc_dir="/content/build/share/khaiii")

n_tags = ['NNG', 'NNP', 'NNB']#, 'VV', "VA" ]
#ex = data['Review'][2]
#bad_ex = data['Review'][4]

def extract_corpus_khaiii(texts):
    extract_corpus = []
    for line in texts:
      if str(line) != 'nan':
        nouns = []

        for word in api.analyze(str(line)):
          for morphs in word.morphs:
            if morphs.tag in n_tags:
              if len(morphs.lex) > 1:
                nouns.append(morphs.lex)
              else:
                continue

        extract_corpus.append(nouns)

    return extract_corpus

extract_corpus_khaiii = extract_corpus_khaiii(data['Review'])

extract_corpus_khaiii

# countvectorize를 위한 역토큰화 진행

detokenized_doc = []

for i in range(len(extract_corpus_khaiii)):
  if extract_corpus_khaiii[i] != []:
    t = ' '.join(extract_corpus_khaiii[i])
    detokenized_doc.append(t)

detokenized_doc[:10]

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=2)

vect = vectorizer.fit_transform(detokenized_doc)

np.sum(vect.toarray()[0])

from sklearn.decomposition import LatentDirichletAllocation

lda = LatentDirichletAllocation(n_components=6, random_state=0)
lda.fit_transform(vect)

terms = vectorizer.get_feature_names()

def get_topics(components, feature_names, n=10):
  temp = []
  topic_result = {}
  for idx, topic in enumerate(components):
      topic_result["Topic "+str(idx+1)] =  [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]]

  return topic_result

def print_topics(components, feature_names, n=10):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], np.log(topic[i]).round(2)) for i in topic.argsort()[:-n - 1:-1]])

print_topics(lda.components_, terms)

np.sum(lda.components_[0])

n_min_df = [0.0001, 0.001, 0.01, 0.1, 1, 2, 3]
n_topic = [6, 8, 10, 12]

result = pd.DataFrame()

for min_df in n_min_df:
  temp_topic = {}
  vectorizer = CountVectorizer(min_df=min_df)
  vect = vectorizer.fit_transform(detokenized_doc)
  terms = vectorizer.get_feature_names()

  for topics in n_topic:
    lda = LatentDirichletAllocation(n_components=topics)
    lda.fit_transform(vect.toarray())
    temp_topic = get_topics(lda.components_, terms)
    temp_topic['n_df'] = min_df
    temp_topic['n_topic'] = topics


    temp = pd.DataFrame.from_dict(temp_topic)
    result = pd.concat([result, temp], axis=0)

result.keys()

result = result[['n_df', 'n_topic', 'Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6','Topic 7', 'Topic 8', 'Topic 9', 'Topic 10','Topic 11', 'Topic 12']]

result

result.to_csv('Topic 결과.csv', encoding='utf-8')

!pip install gensim

from gensim.matutils import Sparse2Corpus

corpus = Sparse2Corpus(vect.T)

id2word = dict(enumerate(vectorizer.get_feature_names()))

id2word[440]

from gensim.models.ldamodel import LdaModel
model = LdaModel(
    corpus=corpus,
    id2word=id2word,
    
    num_topics=8,
    #passes=3,
    #iterations=100,
    
    random_state=1)

model.show_topic(0)

model.get_document_topics()

log_pp = model.log_perplexity(corpus)
log_pp

2 ** -log_pp

import re
token_re = re.compile(vectorizer.token_pattern)

pd.isnull(data.Review[0])

lst = []
for i in range(len(data)) :
  if pd.isnull(data.Review[i]) == True :
    lst.append(i)

#df = data.drop(lst)
#len(data)-len(lst)
display(df)

words = set(vectorizer.get_feature_names())
texts = []
for review in df['Review']:
    text = []
    for word in token_re.findall(review):
        word = word.lower()
        if word in words:
            text.append(word)
    texts.append(text)

from gensim.corpora.dictionary import Dictionary
dic = Dictionary()
dic.token2id = {t: i for i, t in enumerate(vectorizer.get_feature_names())}

from gensim.models import CoherenceModel

coh = CoherenceModel(model=model, texts=texts, dictionary=dic, coherence='c_v')
coh.get_coherence()

